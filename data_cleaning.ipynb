{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Projet Integre</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:yellow\">Description des fichiers</span>\n",
    "\n",
    "### publication (id_publication, date_pub, nbr_publication, article_title) \n",
    "\n",
    "author_publication (id_author, id_publication)  \n",
    "author (id_author, name_author, nbr_publication)\n",
    "\n",
    "keyword_publication (id_publication, keyword, nbr_use_keyword)  \n",
    "keyword (keyword, nbr_used)\n",
    "\n",
    "year_publication (id_publication, id_year)  \n",
    "year (id_year, year)\n",
    "\n",
    "publication_venue (id_publication, id_venue)  \n",
    "venue (id_venue, name_venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:yellow\">Data Cleaning (bibliothèques re, string), text preprocessing techniques</span>\n",
    "\n",
    "  * suppression des colonnes inutiles\n",
    "  *   mettre les titres en minuscule\n",
    "  *   supprimer la ponctuation\n",
    "  *   supprimer les valeurs numériques\n",
    "  *   supprimer les élément qui n'ont aucune signification (balise et autre spéciaux)\n",
    "  *   tokénisation (uni-gram d'abord)\n",
    "  *   suppression des mots vides (stop words) \n",
    "\n",
    "\n",
    "###  Advanced data cleaning : \n",
    "\n",
    "  * racinisation (stemming) / lemmatisation \n",
    "  * tokénisation avancée : bi-grams ou tri-grams  \n",
    "  * gestion des erreurs de frappe ? \n",
    "\n",
    "\n",
    "\n",
    "### Organisation des données (sklearn, countvectorizer)\n",
    "   * corpus (ensemble des titres d'article), \n",
    "   * création de la matrice documents-termes (sklearn, countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option d'affichage pour une meilleur visibilité\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.min_rows', 25)\n",
    "pd.set_option('display.max_rows', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des fichiers csv\n",
    "\n",
    "author=pd.read_csv('author.csv',sep=',',encoding='cp1252') # (id_author, name_author)\n",
    "publication=pd.read_csv('publication.csv',sep=',',encoding='cp1252') # Fichier des\n",
    "del publication['categorie'] # suppression de la colonne qui ne contient que des proceeding\n",
    "keyword=pd.read_csv('keyword.csv',sep=',',encoding='cp1252')\n",
    "year=pd.read_csv('year.csv',sep=',',encoding='cp1252')\n",
    "\n",
    "\n",
    "author_publication=pd.read_csv('publication_author.csv',sep=',',encoding='cp1252')\n",
    "keyword_publication=pd.read_csv('Publication_keywords.csv',sep=',',encoding='cp1252')\n",
    "year_publication=pd.read_csv('publication_year.csv',sep=',',encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Pink\">Travail sur le fichier publication</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Suppression des doublons et préparation des données avant nettoyage = titre correct des publications</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8205</th>\n",
       "      <td>conf/aabi/LueckmannBKM18</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>4</td>\n",
       "      <td>Likelihood-free inference with emulator networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52438</th>\n",
       "      <td>conf/birthday/0001ZS18</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>3</td>\n",
       "      <td>Network Security Metrics: From Known Vulnerabilities to Zero Day Attacks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52441</th>\n",
       "      <td>conf/birthday/2017brinksma</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>3</td>\n",
       "      <td>ModelEd. TestEd. TrustEd - Essays Dedicated to Ed Brinksma on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52443</th>\n",
       "      <td>conf/birthday/2017downey</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>6</td>\n",
       "      <td>Computability and Complexity - Essays Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52445</th>\n",
       "      <td>conf/birthday/2017larsen</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>6</td>\n",
       "      <td>Models. Algorithms. Logics and Tools - Essays Dedicated to Kim Guldstrand Larsen on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141352</th>\n",
       "      <td>conf/eusipco/0042HH19</td>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>3</td>\n",
       "      <td>Sensor Selection and Rate Distribution Based Beamforming in Wireless Acoustic Sensor Networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334051</th>\n",
       "      <td>conf/konvens/0002W16</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2</td>\n",
       "      <td>A Study on Gaps and Syntactic Boundaries in Spoken Interaction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367994</th>\n",
       "      <td>conf/nips/2016tsw</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>5</td>\n",
       "      <td>Proceedings of the NIPS 2016 Time Series Workshop. co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). Barcelona. Spain. December 9. 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id_publication    date_pub  nbr_authors  \\\n",
       "8205      conf/aabi/LueckmannBKM18  2019-07-12            4   \n",
       "52438       conf/birthday/0001ZS18  2018-12-02            3   \n",
       "52441   conf/birthday/2017brinksma  2019-05-14            3   \n",
       "52443     conf/birthday/2017downey  2019-05-14            6   \n",
       "52445     conf/birthday/2017larsen  2019-05-14            6   \n",
       "141352       conf/eusipco/0042HH19  2019-11-25            3   \n",
       "334051        conf/konvens/0002W16  2019-11-28            2   \n",
       "367994           conf/nips/2016tsw  2019-05-29            5   \n",
       "\n",
       "                                                                                                                                                                                 article_title  \n",
       "8205                                                                                                                                         Likelihood-free inference with emulator networks.  \n",
       "52438                                                                                                                Network Security Metrics: From Known Vulnerabilities to Zero Day Attacks.  \n",
       "52441                                                                                          ModelEd. TestEd. TrustEd - Essays Dedicated to Ed Brinksma on the Occasion of His 60th Birthday  \n",
       "52443                                                                                 Computability and Complexity - Essays Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday  \n",
       "52445                                                                    Models. Algorithms. Logics and Tools - Essays Dedicated to Kim Guldstrand Larsen on the Occasion of His 60th Birthday  \n",
       "141352                                                                                          Sensor Selection and Rate Distribution Based Beamforming in Wireless Acoustic Sensor Networks.  \n",
       "334051                                                                                                                         A Study on Gaps and Syntactic Boundaries in Spoken Interaction.  \n",
       "367994  Proceedings of the NIPS 2016 Time Series Workshop. co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). Barcelona. Spain. December 9. 2016  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication[publication.duplicated()] # Doublons sur les lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication=pd.DataFrame.drop_duplicates(publication) # suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>conf/3dim/KimMQ18</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>3</td>\n",
       "      <td>Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>conf/3dor/LangenfeldACCDD18</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>23</td>\n",
       "      <td>Protein Shape Retrieval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>conf/3dor/SongCCHKLMQSTL17</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>11</td>\n",
       "      <td>Protein Shape Retrieval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>conf/3pgcic/SabaKIPAAJ17</td>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>7</td>\n",
       "      <td>Home Energy Management Using Social Spider and Bacterial Foraging Algorithm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>conf/ACISicis/HaqbeenIAO18</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>4</td>\n",
       "      <td>Traffic Adaptive Hybrid MAC with QoS Driven Energy Efficiency for WSNs Through Joint Dynamic Scheduling Mode.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>conf/ACISicis/HuX17</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2</td>\n",
       "      <td>A machinima system based on network game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>conf/ACISicis/HuX17a</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2</td>\n",
       "      <td>A machinima system based on network game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>conf/ACISicis/KawaguchiIU19</td>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>3</td>\n",
       "      <td>Learning Neural Circuit by AC Operation and Frequency Signal Output.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>conf/ACISicis/WangCZZ17</td>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>4</td>\n",
       "      <td>Real-time image detail enhancement implementing on multi-core DSP platform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>conf/ACISicis/WangCZZ17a</td>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>4</td>\n",
       "      <td>Real-time image detail enhancement implementing on multi-core DSP platform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>conf/ACISicis/WangL17</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2</td>\n",
       "      <td>A parallel image processing platform based on multi-core DSP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>conf/ACISicis/WangL17a</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>2</td>\n",
       "      <td>A parallel image processing platform based on multi-core DSP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468876</th>\n",
       "      <td>conf/www/Getoor19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>1</td>\n",
       "      <td>Responsible Data Science.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468893</th>\n",
       "      <td>conf/www/Gomez-Rodriguez17</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Distilling Information Reliability and Source Trustworthiness from Digital Traces.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468930</th>\n",
       "      <td>conf/www/GuptaKDOJ19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>5</td>\n",
       "      <td>A/B Testing at Scale: Accelerating Software Innovation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468966</th>\n",
       "      <td>conf/www/HeindorfSEP19</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>4</td>\n",
       "      <td>Debiasing Vandalism Detection Models at Wikidata.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469095</th>\n",
       "      <td>conf/www/KenthapadiMT19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>3</td>\n",
       "      <td>Privacy-preserving Data Mining in Industry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469348</th>\n",
       "      <td>conf/www/MantrachS19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2</td>\n",
       "      <td>Enlisting the Public to Build a Healthier Web Information Commons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469735</th>\n",
       "      <td>conf/www/TabibianVFSSG17</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>6</td>\n",
       "      <td>Distilling Information Reliability and Source Trustworthiness from Digital Traces.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469814</th>\n",
       "      <td>conf/www/VrandecicL19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2</td>\n",
       "      <td>The First International Workshop on Knowledge Graph Technology and Applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469871</th>\n",
       "      <td>conf/www/Wardle19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>1</td>\n",
       "      <td>Enlisting the Public to Build a Healthier Web Information Commons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469904</th>\n",
       "      <td>conf/www/WuY19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2</td>\n",
       "      <td>Deep Chit-Chat: Deep Learning for Chatbots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469994</th>\n",
       "      <td>conf/www/Zhang17b</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470030</th>\n",
       "      <td>conf/www/ZhangSWSTS17</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>6</td>\n",
       "      <td>Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1907 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id_publication    date_pub  nbr_authors  \\\n",
       "98                conf/3dim/KimMQ18  2019-07-23            3   \n",
       "252     conf/3dor/LangenfeldACCDD18  2019-01-29           23   \n",
       "275      conf/3dor/SongCCHKLMQSTL17  2017-11-24           11   \n",
       "572        conf/3pgcic/SabaKIPAAJ17  2019-01-08            7   \n",
       "997      conf/ACISicis/HaqbeenIAO18  2019-10-19            4   \n",
       "1012            conf/ACISicis/HuX17  2017-07-07            2   \n",
       "1013           conf/ACISicis/HuX17a  2017-07-07            2   \n",
       "1029    conf/ACISicis/KawaguchiIU19  2019-08-08            3   \n",
       "1160        conf/ACISicis/WangCZZ17  2019-03-14            4   \n",
       "1161       conf/ACISicis/WangCZZ17a  2019-03-14            4   \n",
       "1168          conf/ACISicis/WangL17  2017-07-07            2   \n",
       "1169         conf/ACISicis/WangL17a  2017-07-07            2   \n",
       "...                             ...         ...          ...   \n",
       "468876            conf/www/Getoor19  2019-05-16            1   \n",
       "468893   conf/www/Gomez-Rodriguez17  2018-11-06            1   \n",
       "468930         conf/www/GuptaKDOJ19  2019-05-16            5   \n",
       "468966       conf/www/HeindorfSEP19  2019-12-27            4   \n",
       "469095      conf/www/KenthapadiMT19  2019-05-16            3   \n",
       "469348         conf/www/MantrachS19  2019-05-16            2   \n",
       "469735     conf/www/TabibianVFSSG17  2018-11-14            6   \n",
       "469814        conf/www/VrandecicL19  2019-05-16            2   \n",
       "469871            conf/www/Wardle19  2019-05-16            1   \n",
       "469904               conf/www/WuY19  2019-05-16            2   \n",
       "469994            conf/www/Zhang17b  2018-11-06            1   \n",
       "470030        conf/www/ZhangSWSTS17  2018-11-06            6   \n",
       "\n",
       "                                                                                                        article_title  \n",
       "98                                                  Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes.  \n",
       "252                                                                                          Protein Shape Retrieval.  \n",
       "275                                                                                          Protein Shape Retrieval.  \n",
       "572                                      Home Energy Management Using Social Spider and Bacterial Foraging Algorithm.  \n",
       "997     Traffic Adaptive Hybrid MAC with QoS Driven Energy Efficiency for WSNs Through Joint Dynamic Scheduling Mode.  \n",
       "1012                                                                        A machinima system based on network game.  \n",
       "1013                                                                        A machinima system based on network game.  \n",
       "1029                                             Learning Neural Circuit by AC Operation and Frequency Signal Output.  \n",
       "1160                                      Real-time image detail enhancement implementing on multi-core DSP platform.  \n",
       "1161                                      Real-time image detail enhancement implementing on multi-core DSP platform.  \n",
       "1168                                                    A parallel image processing platform based on multi-core DSP.  \n",
       "1169                                                    A parallel image processing platform based on multi-core DSP.  \n",
       "...                                                                                                               ...  \n",
       "468876                                                                                      Responsible Data Science.  \n",
       "468893                             Distilling Information Reliability and Source Trustworthiness from Digital Traces.  \n",
       "468930                                                        A/B Testing at Scale: Accelerating Software Innovation.  \n",
       "468966                                                              Debiasing Vandalism Detection Models at Wikidata.  \n",
       "469095                                                                    Privacy-preserving Data Mining in Industry.  \n",
       "469348                                             Enlisting the Public to Build a Healthier Web Information Commons.  \n",
       "469735                             Distilling Information Reliability and Source Trustworthiness from Digital Traces.  \n",
       "469814                               The First International Workshop on Knowledge Graph Technology and Applications.  \n",
       "469871                                             Enlisting the Public to Build a Healthier Web Information Commons.  \n",
       "469904                                                                    Deep Chit-Chat: Deep Learning for Chatbots.  \n",
       "469994                                               Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.  \n",
       "470030                                               Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.  \n",
       "\n",
       "[1907 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication[publication['article_title'].duplicated(keep=False)] # doublons aussi sur les colonnes !\n",
    "\n",
    "# Note : certaines publications de mettre titre ont deux id_publication qui différent d'une lettre et certaines ont deux dates différentes \n",
    "# pour un même id_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_rows = publication[publication['article_title'].duplicated(keep='first')]\n",
    "publication.drop(delete_rows.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = publication[\"article_title\"].str.lower() # articles en minuscle\n",
    "article_titles = article_titles.tolist() # conversion en liste\n",
    "\n",
    "list_id_publication = list(publication['id_publication'])\n",
    "\n",
    "article_titles = dict(zip(list_id_publication,article_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">1er nettoyage : suppression des balises et des caractères indésirables et autres aberrations</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le premier nettoyage consiste à supprimer les caractères et les balises qui ne devraient pas se trouver dans les titres des publications (erreurs d'exportation des données)\n",
    "\n",
    "# Etant donné la diversité des erreurs une perte d'information est possible \n",
    "#   -> ex: on veut supprimer tout ce qui se trouve des parenthèses : on ne sait pas ce qu'il y aura\n",
    "#   dans la parenthèses. La plus part du temps ce sont des abréviations mais ce peut être un mot clef (bien que rare).. Il faut faire des compromis.\n",
    "\n",
    "for i in article_titles:\n",
    "    article_titles[i]=re.sub('\\[.*?\\]','',article_titles[i]) # suppression de toutes les expressions entre crochets\n",
    "    article_titles[i]=re.sub('\\{.*?\\}','',article_titles[i]) # suppression de toutes les expressions entre accolades\n",
    "    article_titles[i]=re.sub('\\(.*?\\)','',article_titles[i]) # suppression de toutes les expressions entre parenthèses\n",
    "    article_titles[i]=re.sub('<.+>','',article_titles[i])    # suppression de toute la chaine qui se trouve entre un '<' et un '>' (1ère et dernière occurence)\n",
    "    article_titles[i]=re.sub('\\$.+\\$','',article_titles[i])  # suppression de toute la chaine qui se trouve entre un '$' et un '$' (1ère et dernière occurence)\n",
    "    article_titles[i]=re.sub('\\S*[é,è,\\^,_,&,:,@,#,$,\\\\\\]\\S*',' ',article_titles[i]) # suppression de tous les mots contenant un @ et #.. (@ et # seule inclus aussi)\n",
    "    article_titles[i]=re.sub('[.,\\,,!,?,%,*,§,²,~,\\{,\\}]',' ',article_titles[i]) # suppression des caractères spéciaux\n",
    "    article_titles[i]=re.sub('[;,+,/]',' ',article_titles[i])\n",
    "    article_titles[i]=re.sub(' +', ' ', article_titles[i]) # suppression des espaces en trop\n",
    "    \n",
    "# reste à gérer les guillemets simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = article_titles\n",
    "# reste à supprimer les th après les chigg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_titles=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching = [s for s in article_titles if '^' in s]\n",
    "#matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> 2ème nettoyage : Suppression des tirets, des caractères seuls, des stop words et des chaines de caractères référencant des dates ou des pays</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_list import countries_for_language # pip install country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    article_titles[i]=re.sub('-','',article_titles[i]) # remplacement des tirets par rien # choix qui se discute\n",
    "    article_titles[i]=re.sub('(\\s\\d+\\s|^\\d+\\s|\\s\\d+$)','',article_titles[i]) # suppression de tous les chiffres qui sont seuls\n",
    "    article_titles[i]=re.sub('(\\s(.)\\s)|(^.\\s)|(\\s.$)','',article_titles[i]) #suppression des caractères d'une lettre\n",
    "    article_titles[i]=re.sub('(\\'s)','',article_titles[i]) # suppression des 's\n",
    "    article_titles[i]=re.sub('\\S*((january)|(february)|(march)|(april)|(may)|(june)|(july)|(august)|(september)|(october)|(november)|(december))\\S*','',article_titles[i]) # suppression des expressions qui continnent un mois\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on veut supprimer les références à des pays\n",
    "\n",
    "country_exclusion = [x.lower() for x in list(countries.values())]\n",
    "country_exclusion = '|'.join(country_exclusion)\n",
    "for i in article_titles:\n",
    "    article_titles[i]=re.sub(country_exclusion,'',article_titles[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des mots vides\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords # pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'most', 'toward', 'another', 'thru', 'what', 'become', 'noone', 'other', 'found', 'everything', 'empty', 'ten', 'beforehand', 'yet', 'thereby', 'her', 'this', 'something', 'mine', 'fifty', 'might', 'sixty', 'may', 'hundred', 'these', 'amongst', 'alone', 'now', 'some', 'system', 'as', 'without', 'few', 'five', 'by', 'elsewhere', 'thus', 'why', 'wherein', 'who', 'does', 'etc', 'didn', 'or', 'due', 'various', 'their', 'namely', 'are', 'his', 'being', 'if', 'used', 'never', 'an', 'least', 'hence', 'whence', 'show', 'inc', 'therefore', 'doing', 'bill', 'hereafter', 'thick', 'eleven', 'bottom', 'itself', 'de', 'more', 'we', 'take', 'therein', 'up', 'somewhere', 'every', 'anyhow', 'both', 'he', 'should', 'seemed', 'regarding', 'only', 'yourself', 'own', 'throughout', 'nobody', 'whereafter', 'last', 'was', 'cry', 'also', 'twelve', 'just', 'when', 'thereafter', 'off', 'before', 'describe', 'about', 'anything', 'km', 'each', 'made', 'after', 'though', 'very', 'our', 'whereby', 'go', 'first', 'say', 'find', 'using', 'meanwhile', 'less', 'everyone', 'via', 'whoever', 'full', 'upon', 'kg', 'co', 'down', 'rather', 'hasnt', 'serious', 'am', 'along', 'together', 'within', 'were', 'that', 'whole', 'none', 'front', 'mill', 'much', 'quite', 'top', 'such', 'others', 'themselves', 'has', 'former', 'whenever', 'sometime', 'couldnt', 'often', 'nevertheless', 'three', 'neither', 'for', 'ie', 'me', 'else', 'you', 'anyone', 'further', 'us', 'anyway', 'them', 'interest', 'of', 'on', 'to', 'in', 'all', 'been', 'whither', 'is', 'whereas', 'several', 'under', 'him', 'here', 'don', 'except', 'out', 'becoming', 'your', 'done', 'although', 're', 'eight', 'with', 'cannot', 'nor', 'beyond', 'call', 'over', 'until', 'than', 'amount', 'mostly', 'unless', 'four', 'sometimes', 'yourselves', 'name', 'make', 'thin', 'there', 'into', 'whether', 'twenty', 'once', 'above', 'can', 'the', 'see', 'beside', 'not', 'do', 'same', 'between', 'afterwards', 'it', 'against', 'two', 'became', 'somehow', 'behind', 'amoungst', 'where', 'well', 'either', 'indeed', 'so', 'had', 'then', 'below', 'put', 'could', 'next', 'always', 'onto', 'almost', 'which', 'but', 'computer', 'please', 'already', 'perhaps', 'however', 'fill', 'eg', 'cant', 'hereby', 'even', 'per', 'keep', 'how', 'yours', 'whatever', 'anywhere', 'forty', 'have', 'whom', 'again', 'ever', 'they', 'moreover', 'hers', 'she', 'would', 'ourselves', 'nine', 'must', 'a', 'ltd', 'towards', 'and', 'someone', 'among', 'latterly', 'six', 'otherwise', 'through', 'ours', 'hereupon', 'its', 'no', 'will', 'many', 'latter', 'did', 'at', 'thence', 'myself', 'since', 'fire', 'my', 'too', 'enough', 'during', 'nowhere', 'get', 'because', 'everywhere', 'those', 'around', 'across', 'part', 'whereupon', 'still', 'herein', 'nothing', 'any', 'whose', 'seem', 'con', 'one', 'thereupon', 'seeming', 'himself', 'besides', 'i', 'third', 'move', 'while', 'wherever', 'herself', 'fifteen', 'side', 'really', 'detail', 'becomes', 'be', 'formerly', 'seems', 'from', 'un', 'give', 'doesn', 'back', 'sincere'})\n"
     ]
    }
   ],
   "source": [
    "# la suppression des mots vides est différente selon les différentes librairy. Il est néanmoins possible de modifier cette liste.\n",
    "\n",
    "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    article_titles[i]=remove_stopwords(article_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/3dim/AbarghoueiB19': 'complete estimate themultitask approach depth completion monocular depth estimation',\n",
       " 'conf/3dim/AbbeloosCCTD17': '3d object discovery modeling single rgbd images containing multiple object instances',\n",
       " 'conf/3dim/AbrevayaWB18': 'spatiotemporal modeling efficient registration dynamic 3d faces',\n",
       " 'conf/3dim/AdanR19': 'reconstruction asis semantic 3d models unorganised storehouses',\n",
       " 'conf/3dim/AhmedMG17': 'point cloud registration virtual points implicit quadric surface intersections',\n",
       " 'conf/3dim/AhujaM19': 'natural language grounded pose forecasting',\n",
       " 'conf/3dim/Al-HamiL17': 'reconstructing 3d human poses keyword based image database query',\n",
       " 'conf/3dim/AlexandrovPZV17': 'high dynamic range slam mapaware exposure time control',\n",
       " 'conf/3dim/AliGS18': 'gravitational approach nonrigid point set registration',\n",
       " 'conf/3dim/AlldieckMXTP18': 'detailed human avatars monocular video',\n",
       " 'conf/3dim/AlzugarayC18': 'efficient asynchronous corner tracker event cameras',\n",
       " 'conf/3dim/AlzugarayC19': 'asynchronous multihypothesis tracking features event cameras',\n",
       " 'conf/3dim/AndraghettiMDLP19': 'enhancing selfsupervised monocular depth estimation traditional visual odometry',\n",
       " 'conf/3dim/AnisimovS17': 'fast efficient depth map estimation light fields',\n",
       " 'conf/3dim/AnsariGS17': 'scalable dense monocular surface reconstruction',\n",
       " 'conf/3dim/ArmandoFB19': 'adaptive mesh texture multiview appearance modeling',\n",
       " 'conf/3dim/BachmannSFR19': 'motion capture pantilt cameras unknown orientation',\n",
       " 'conf/3dim/BalashovaSWTCF18': 'structureaware shape synthesis',\n",
       " 'conf/3dim/BatsosM18': 'recurrent residual cnn architecture disparity map enhancement',\n",
       " 'conf/3dim/BednarikFS18': 'learning reconstruct textureless deformable surfaces fromsingle view'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:20]) # 20 premiers éléments du dictionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEP : regarder comment éviter encore certaines erreurs (ex: 'themultitask' dans la 1ère valeur du dict) ??) + voir les oublies de certains caractères spéciaux + optimisation de la suppression d'autres mots inutiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Tokénisation en vue de la création de la matrice document-termes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Romain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    article_titles[i] = word_tokenize(article_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/3dim/AbarghoueiB19': ['complete',\n",
       "  'estimate',\n",
       "  'themultitask',\n",
       "  'approach',\n",
       "  'depth',\n",
       "  'completion',\n",
       "  'monocular',\n",
       "  'depth',\n",
       "  'estimation'],\n",
       " 'conf/3dim/AbbeloosCCTD17': ['3d',\n",
       "  'object',\n",
       "  'discovery',\n",
       "  'modeling',\n",
       "  'single',\n",
       "  'rgbd',\n",
       "  'images',\n",
       "  'containing',\n",
       "  'multiple',\n",
       "  'object',\n",
       "  'instances'],\n",
       " 'conf/3dim/AbrevayaWB18': ['spatiotemporal',\n",
       "  'modeling',\n",
       "  'efficient',\n",
       "  'registration',\n",
       "  'dynamic',\n",
       "  '3d',\n",
       "  'faces'],\n",
       " 'conf/3dim/AdanR19': ['reconstruction',\n",
       "  'asis',\n",
       "  'semantic',\n",
       "  '3d',\n",
       "  'models',\n",
       "  'unorganised',\n",
       "  'storehouses'],\n",
       " 'conf/3dim/AhmedMG17': ['point',\n",
       "  'cloud',\n",
       "  'registration',\n",
       "  'virtual',\n",
       "  'points',\n",
       "  'implicit',\n",
       "  'quadric',\n",
       "  'surface',\n",
       "  'intersections']}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire tokénisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Racinisation / Lemmatisation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compléter par Nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
