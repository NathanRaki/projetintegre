{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCpeKZRL_Nml"
   },
   "source": [
    "<h1><center>Projet Integre</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKz_F8dk_Nmm"
   },
   "source": [
    "## <span style=\"color:yellow\">Description des fichiers</span>\n",
    "\n",
    "### publication (id_publication, date_pub, nbr_publication, article_title) \n",
    "\n",
    "author_publication (id_author, id_publication)  \n",
    "author (id_author, name_author, nbr_publication)\n",
    "\n",
    "keyword_publication (id_publication, keyword, nbr_use_keyword)  \n",
    "keyword (keyword, nbr_used)\n",
    "\n",
    "year_publication (id_publication, id_year)  \n",
    "year (id_year, year)\n",
    "\n",
    "publication_venue (id_publication, id_venue)  \n",
    "venue (id_venue, name_venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVvl5dM8_Nmn"
   },
   "source": [
    "## <span style=\"color:yellow\">Data Cleaning (bibliothèques re, string), text preprocessing techniques</span>\n",
    "\n",
    "  * suppression des colonnes inutiles\n",
    "  *   mettre les titres en minuscule\n",
    "  *   supprimer la ponctuation\n",
    "  *   supprimer les valeurs numériques\n",
    "  *   supprimer les élément qui n'ont aucune signification (balise et autre spéciaux)\n",
    "  *   tokénisation (uni-gram d'abord)\n",
    "  *   suppression des mots vides (stop words) \n",
    "\n",
    "\n",
    "###  Advanced data cleaning : \n",
    "\n",
    "  * racinisation (stemming) / lemmatisation \n",
    "  * tokénisation avancée : bi-grams ou tri-grams  \n",
    "  * gestion des erreurs de frappe ? \n",
    "\n",
    "\n",
    "\n",
    "### Organisation des données (sklearn, countvectorizer)\n",
    "   * corpus (ensemble des titres d'article), \n",
    "   * création de la matrice documents-termes (sklearn, countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0-fomjQB_Nmn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "\n",
    "# librairies qui seront utiles ensuite\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# option d'affichage pour une meilleur visibilité\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#pd.set_option('display.min_rows', 25)\n",
    "#pd.set_option('display.max_rows', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JyrXoV9c_Nmq"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(dico, filename) :\n",
    "    with open(\"dict/\"+filename+\".pickle\", 'wb') as handle:\n",
    "        pickle.dump(dico, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_dict(filename) :\n",
    "    with open(\"dict/\"+filename+\".pickle\", 'rb') as handle:\n",
    "        return(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EJ-OnyBn_Nms"
   },
   "outputs": [],
   "source": [
    "# Lecture des fichiers csv\n",
    "\n",
    "author=pd.read_csv('data/author.csv',sep=',',encoding='cp1252') # (id_author, name_author)\n",
    "publication=pd.read_csv('data/publication.csv',sep=',',encoding='cp1252') # Fichier des\n",
    "del publication['categorie'] # suppression de la colonne qui ne contient que des proceeding\n",
    "keyword=pd.read_csv('data/keyword.csv',sep=',',encoding='cp1252')\n",
    "year=pd.read_csv('data/year.csv',sep=',',encoding='cp1252')\n",
    "\n",
    "\n",
    "author_publication=pd.read_csv('data/publication_author.csv',sep=',',encoding='cp1252')\n",
    "keyword_publication=pd.read_csv('data/Publication_keywords.csv',sep=',',encoding='cp1252')\n",
    "year_publication=pd.read_csv('data/publication_year.csv',sep=',',encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ryQZ5D0O_Nmu",
    "outputId": "e6678cba-6226-4feb-c60b-dae47a575add",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.shape[0] # 17585 mots clefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbtSSlWd_Nmy"
   },
   "source": [
    "### <span style=\"color:Pink\">Travail sur le fichier publication</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI2d37vv_Nmy"
   },
   "source": [
    "#### <span style=\"color:red\">Suppression des doublons et préparation des données avant nettoyage = titre correct des publications</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yxrcG9Ek_Nmz",
    "outputId": "a35cf182-504a-4b10-eee5-1752974c45dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conf/3dim/AbarghoueiB19</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>2</td>\n",
       "      <td>To Complete or to Estimate. That is the Question: A Multi-Task Approach to Depth Completion and Monocular Depth Estimation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf/3dim/AbbeloosCCTD17</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>5</td>\n",
       "      <td>3D Object Discovery and Modeling Using Single RGB-D Images Containing Multiple Object Instances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/3dim/AbrevayaWB18</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>3</td>\n",
       "      <td>Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf/3dim/AdanR19</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>2</td>\n",
       "      <td>Reconstruction of As-is Semantic 3D Models of Unorganised Storehouses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf/3dim/AhmedMG17</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>3</td>\n",
       "      <td>Point Cloud Registration with Virtual Interest Points from Implicit Quadric Surface Intersections.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_publication    date_pub  nbr_authors  \\\n",
       "0   conf/3dim/AbarghoueiB19  2019-11-05            2   \n",
       "1  conf/3dim/AbbeloosCCTD17  2018-07-03            5   \n",
       "2    conf/3dim/AbrevayaWB18  2018-10-26            3   \n",
       "3         conf/3dim/AdanR19  2019-11-05            2   \n",
       "4       conf/3dim/AhmedMG17  2019-09-25            3   \n",
       "\n",
       "                                                                                                                 article_title  \n",
       "0  To Complete or to Estimate. That is the Question: A Multi-Task Approach to Depth Completion and Monocular Depth Estimation.  \n",
       "1                             3D Object Discovery and Modeling Using Single RGB-D Images Containing Multiple Object Instances.  \n",
       "2                                                      Spatiotemporal Modeling for Efficient Registration of Dynamic 3D Faces.  \n",
       "3                                                       Reconstruction of As-is Semantic 3D Models of Unorganised Storehouses.  \n",
       "4                           Point Cloud Registration with Virtual Interest Points from Implicit Quadric Surface Intersections.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication.head() # Aperçu du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rTGf7w_4_Nm1",
    "outputId": "c638b6a9-3c63-4e90-98f3-1d84886cfc03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8205</th>\n",
       "      <td>conf/aabi/LueckmannBKM18</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>4</td>\n",
       "      <td>Likelihood-free inference with emulator networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52438</th>\n",
       "      <td>conf/birthday/0001ZS18</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>3</td>\n",
       "      <td>Network Security Metrics: From Known Vulnerabilities to Zero Day Attacks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52441</th>\n",
       "      <td>conf/birthday/2017brinksma</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>3</td>\n",
       "      <td>ModelEd. TestEd. TrustEd - Essays Dedicated to Ed Brinksma on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52443</th>\n",
       "      <td>conf/birthday/2017downey</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>6</td>\n",
       "      <td>Computability and Complexity - Essays Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52445</th>\n",
       "      <td>conf/birthday/2017larsen</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>6</td>\n",
       "      <td>Models. Algorithms. Logics and Tools - Essays Dedicated to Kim Guldstrand Larsen on the Occasion of His 60th Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141352</th>\n",
       "      <td>conf/eusipco/0042HH19</td>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>3</td>\n",
       "      <td>Sensor Selection and Rate Distribution Based Beamforming in Wireless Acoustic Sensor Networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334051</th>\n",
       "      <td>conf/konvens/0002W16</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2</td>\n",
       "      <td>A Study on Gaps and Syntactic Boundaries in Spoken Interaction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367994</th>\n",
       "      <td>conf/nips/2016tsw</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>5</td>\n",
       "      <td>Proceedings of the NIPS 2016 Time Series Workshop. co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). Barcelona. Spain. December 9. 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id_publication    date_pub  nbr_authors  \\\n",
       "8205      conf/aabi/LueckmannBKM18  2019-07-12            4   \n",
       "52438       conf/birthday/0001ZS18  2018-12-02            3   \n",
       "52441   conf/birthday/2017brinksma  2019-05-14            3   \n",
       "52443     conf/birthday/2017downey  2019-05-14            6   \n",
       "52445     conf/birthday/2017larsen  2019-05-14            6   \n",
       "141352       conf/eusipco/0042HH19  2019-11-25            3   \n",
       "334051        conf/konvens/0002W16  2019-11-28            2   \n",
       "367994           conf/nips/2016tsw  2019-05-29            5   \n",
       "\n",
       "                                                                                                                                                                                 article_title  \n",
       "8205                                                                                                                                         Likelihood-free inference with emulator networks.  \n",
       "52438                                                                                                                Network Security Metrics: From Known Vulnerabilities to Zero Day Attacks.  \n",
       "52441                                                                                          ModelEd. TestEd. TrustEd - Essays Dedicated to Ed Brinksma on the Occasion of His 60th Birthday  \n",
       "52443                                                                                 Computability and Complexity - Essays Dedicated to Rodney G. Downey on the Occasion of His 60th Birthday  \n",
       "52445                                                                    Models. Algorithms. Logics and Tools - Essays Dedicated to Kim Guldstrand Larsen on the Occasion of His 60th Birthday  \n",
       "141352                                                                                          Sensor Selection and Rate Distribution Based Beamforming in Wireless Acoustic Sensor Networks.  \n",
       "334051                                                                                                                         A Study on Gaps and Syntactic Boundaries in Spoken Interaction.  \n",
       "367994  Proceedings of the NIPS 2016 Time Series Workshop. co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). Barcelona. Spain. December 9. 2016  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication[publication.duplicated()] # Doublons sur les lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OzO5UBTR_Nm3"
   },
   "outputs": [],
   "source": [
    "publication=pd.DataFrame.drop_duplicates(publication) # suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1gZRh0xC_Nm5",
    "outputId": "72ca1d2c-6eac-4445-8496-e2058cd1ebc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>conf/3dim/KimMQ18</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>3</td>\n",
       "      <td>Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>conf/3dor/LangenfeldACCDD18</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>23</td>\n",
       "      <td>Protein Shape Retrieval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>conf/3dor/SongCCHKLMQSTL17</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>11</td>\n",
       "      <td>Protein Shape Retrieval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>conf/3pgcic/SabaKIPAAJ17</td>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>7</td>\n",
       "      <td>Home Energy Management Using Social Spider and Bacterial Foraging Algorithm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>conf/ACISicis/HaqbeenIAO18</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>4</td>\n",
       "      <td>Traffic Adaptive Hybrid MAC with QoS Driven Energy Efficiency for WSNs Through Joint Dynamic Scheduling Mode.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469814</th>\n",
       "      <td>conf/www/VrandecicL19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2</td>\n",
       "      <td>The First International Workshop on Knowledge Graph Technology and Applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469871</th>\n",
       "      <td>conf/www/Wardle19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>1</td>\n",
       "      <td>Enlisting the Public to Build a Healthier Web Information Commons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469904</th>\n",
       "      <td>conf/www/WuY19</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>2</td>\n",
       "      <td>Deep Chit-Chat: Deep Learning for Chatbots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469994</th>\n",
       "      <td>conf/www/Zhang17b</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470030</th>\n",
       "      <td>conf/www/ZhangSWSTS17</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>6</td>\n",
       "      <td>Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1907 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id_publication    date_pub  nbr_authors  \\\n",
       "98                conf/3dim/KimMQ18  2019-07-23            3   \n",
       "252     conf/3dor/LangenfeldACCDD18  2019-01-29           23   \n",
       "275      conf/3dor/SongCCHKLMQSTL17  2017-11-24           11   \n",
       "572        conf/3pgcic/SabaKIPAAJ17  2019-01-08            7   \n",
       "997      conf/ACISicis/HaqbeenIAO18  2019-10-19            4   \n",
       "...                             ...         ...          ...   \n",
       "469814        conf/www/VrandecicL19  2019-05-16            2   \n",
       "469871            conf/www/Wardle19  2019-05-16            1   \n",
       "469904               conf/www/WuY19  2019-05-16            2   \n",
       "469994            conf/www/Zhang17b  2018-11-06            1   \n",
       "470030        conf/www/ZhangSWSTS17  2018-11-06            6   \n",
       "\n",
       "                                                                                                        article_title  \n",
       "98                                                  Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes.  \n",
       "252                                                                                          Protein Shape Retrieval.  \n",
       "275                                                                                          Protein Shape Retrieval.  \n",
       "572                                      Home Energy Management Using Social Spider and Bacterial Foraging Algorithm.  \n",
       "997     Traffic Adaptive Hybrid MAC with QoS Driven Energy Efficiency for WSNs Through Joint Dynamic Scheduling Mode.  \n",
       "...                                                                                                               ...  \n",
       "469814                               The First International Workshop on Knowledge Graph Technology and Applications.  \n",
       "469871                                             Enlisting the Public to Build a Healthier Web Information Commons.  \n",
       "469904                                                                    Deep Chit-Chat: Deep Learning for Chatbots.  \n",
       "469994                                               Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.  \n",
       "470030                                               Smart Jump: Automated Navigation Suggestion for Videos in MOOCs.  \n",
       "\n",
       "[1907 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication[publication['article_title'].duplicated(keep=False)] # doublons aussi sur les colonnes !\n",
    "\n",
    "# Note : certaines publications de mettre titre ont deux id_publication qui différent d'une lettre et certaines ont deux dates différentes \n",
    "# pour un même id_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wGb9qQV__Nm7"
   },
   "outputs": [],
   "source": [
    "delete_rows = publication[publication['article_title'].duplicated(keep='first')] # suppression des doublons sur les colonnes \n",
    "publication.drop(delete_rows.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FhI0fAcz_Nm9"
   },
   "outputs": [],
   "source": [
    "publication = publication.loc[np.random.permutation(publication.index)].reset_index(drop=True) # On mélange le dataset, servira par la suite pour d'éventuels tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1oxkNEQq_Nm-",
    "outputId": "87ae8fa4-02a2-49a8-ef27-94e193d10b78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_publication</th>\n",
       "      <th>date_pub</th>\n",
       "      <th>nbr_authors</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conf/ccs/0001HPZ17</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>4</td>\n",
       "      <td>walk2friends: Inferring Social Links from Mobility Profiles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf/mobisys/LiuLLW19</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>4</td>\n",
       "      <td>Real-time Arm Skeleton Tracking and Gesture Inference Tolerant to Missing Wearable Sensors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf/icarcv/LlopartRAK18</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>4</td>\n",
       "      <td>Online Semantic Segmentation and Manipulation of Objects in Task Intelligence for Service Robots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf/icinfa/Liu17</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Computational awareness for learning neural network ensembles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf/gi/MunteanuSSW19</td>\n",
       "      <td>2019-11-14</td>\n",
       "      <td>4</td>\n",
       "      <td>On Coresets for Logistic Regression.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_publication    date_pub  nbr_authors  \\\n",
       "0        conf/ccs/0001HPZ17  2019-08-31            4   \n",
       "1     conf/mobisys/LiuLLW19  2019-10-19            4   \n",
       "2  conf/icarcv/LlopartRAK18  2019-10-19            4   \n",
       "3         conf/icinfa/Liu17  2020-03-06            1   \n",
       "4     conf/gi/MunteanuSSW19  2019-11-14            4   \n",
       "\n",
       "                                                                                       article_title  \n",
       "0                                       walk2friends: Inferring Social Links from Mobility Profiles.  \n",
       "1        Real-time Arm Skeleton Tracking and Gesture Inference Tolerant to Missing Wearable Sensors.  \n",
       "2  Online Semantic Segmentation and Manipulation of Objects in Task Intelligence for Service Robots.  \n",
       "3                                     Computational awareness for learning neural network ensembles.  \n",
       "4                                                               On Coresets for Logistic Regression.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication.head() # dataset mélangé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j98iP7TK_NnA",
    "outputId": "d47949ad-5141-4905-9ee2-8473323440d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469611"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(publication) # nombre d'articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v9-YNbG__NnC"
   },
   "outputs": [],
   "source": [
    "article_titles = publication[\"article_title\"].str.lower() # conversion des articles en minuscle\n",
    "article_titles = article_titles.tolist() # conversion des articles en liste\n",
    "\n",
    "list_id_publication = list(publication['id_publication']) # liste des identifiants\n",
    "\n",
    "article_titles = dict(zip(list_id_publication,article_titles)) # articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PrNu_Wax_NnE"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"original_titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1jOpe-k_NnG"
   },
   "source": [
    "#### <span style=\"color:red\">1er nettoyage : suppression des titres non-anglais, des balises et des caractères indésirables et autres aberrations</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yz75Bttj_NnG",
    "outputId": "eaa2a6a5-9b87-47de-f568-b4987b774d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "1GtFghe3_NnI"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"original_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?????????????????????? (on the use of sequence labeling and matching methods for asr error detection and correction) [in chinese].'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_titles[\"conf/rocling/WuTHKC17\"] # ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "Pz3x1nO3_NnK"
   },
   "outputs": [],
   "source": [
    "# le premier nettoyage consiste à supprimer les caractères et les balises qui ne devraient pas se trouver dans les titres des publications (erreurs d'exportation des données)\n",
    "\n",
    "# Etant donné la diversité des erreurs une perte d'information est possible \n",
    "#   -> ex: on veut supprimer tout ce qui se trouve des parenthèses : on ne sait pas ce qu'il y aura\n",
    "#   dans la parenthèses. La plus part du temps ce sont des abréviations mais ce peut être un mot clef (bien que rare).. Il faut faire des compromis.\n",
    "\n",
    "for i in article_titles:\n",
    "    article_titles[i]=re.sub('\\[.*?\\]',' ',article_titles[i]) # suppression de toutes les expressions entre crochets\n",
    "    article_titles[i]=re.sub('\\{.*?\\}',' ',article_titles[i]) # suppression de toutes les expressions entre accolades\n",
    "    article_titles[i]=re.sub('\\(.*?\\)',' ',article_titles[i]) # suppression de toutes les expressions entre parenthèses\n",
    "    article_titles[i]=re.sub('<.+>',' ',article_titles[i])    # suppression de toute la chaine qui se trouve entre un '<' et un '>' (1ère et dernière occurence)\n",
    "    article_titles[i]=re.sub('\\$.+\\$',' ',article_titles[i])  # suppression de toute la chaine qui se trouve entre un '$' et un '$' (1ère et dernière occurence)\n",
    "    article_titles[i]=re.sub('\\S*[\\^,_,&,:,@,#,$,\\\\\\]\\S*',' ',article_titles[i]) # suppression de tous les mots contenant un @ et #.. (@ et # seule inclus aussi)\n",
    "    article_titles[i]=re.sub('[.,\\,,!,?,%,*,§,²,~,\\{,\\},\\(,\\)]',' ',article_titles[i]) # suppression des caractères spéciaux\n",
    "    article_titles[i]=re.sub('[;,+,/]',' ',article_titles[i])\n",
    "    \n",
    "# reste à gérer les guillemets simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqWVM12j_NnM"
   },
   "outputs": [],
   "source": [
    "corpus = article_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "k6K7enpk_NnQ",
    "outputId": "8569608f-00b6-4480-ac1f-3421127b3dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching = [s for s in article_titles.values() if ')' in s] # test de la présence du mot conference\n",
    "matching[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA5b8iLv_NnS"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "GFZxrOvH_NnT"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"firstclean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "77EUaHoR_NnV"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"firstclean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXZc0vIP_NnW"
   },
   "source": [
    "#### <span style=\"color:orange\">----------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OaYrBsS_NnW"
   },
   "source": [
    "#### <span style=\"color:red\"> 2ème nettoyage : Suppression des tirets, des caractères seuls, des stop words et des chaines de caractères référencant des dates ou des pays</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                           '"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_titles[\"conf/rocling/WuTHKC17\"] # problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "ef0NWUGp_NnX"
   },
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    article_titles[i]=re.sub('-',' ',article_titles[i]) # remplacement par un espace, se discute\n",
    "    article_titles[i]=re.sub(r'\\b\\d+\\b',' ',article_titles[i]) # suppression de tous les chiffres seuls\n",
    "    article_titles[i]=re.sub('\\S*\\d[^gd]\\S*',' ',article_titles[i]) # suppression de tous les chiffres et des chaines contenant un chiffre excepté les lettres d et g (ne supprime pas les g et d en trop)\n",
    "    article_titles[i]=re.sub('\\S*\\d\\w{2,}\\S*',' ',article_titles[i]) # suppression des éventuels mots avec des plusieurs g ou d à la suite (suite de l'expression d'avant)\n",
    "    article_titles[i]=re.sub('\\S*\\w+\\d\\S*',' ',article_titles[i]) # supression des chaines qui précédent un nombre\n",
    "    article_titles[i]=re.sub('(\\'s)|(\\')',' ',article_titles[i]) # suppression des 's ou guillemets simple\n",
    "    article_titles[i]=re.sub('\\S*((january)|(february)|(march)|(april)|(may)|(june)|(july)|(august)|(september)|(october)|(november)|(december))\\S*',' ',article_titles[i]) # suppression des expressions qui continnent un mois\n",
    "    #article_titles[i]=re.sub('([0-9]+)(?:st|nd|rd|th)','',article_titles[i]) # suppression des expression en rapoort avec des dates ou des n-ièmes\n",
    "    article_titles[i]=re.sub('(conference)|(international)',' ',article_titles[i]) # suppression des mots conference et international qui sont particlièrement fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6189"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching = [s for s in article_titles.values() if '3d' in s] # test de la présence du mot conference\n",
    "len(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "QC6pTTXo_NnZ"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"secondclean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "idlcW_ax_Nnb",
    "outputId": "41706c9e-0424-437e-a143-95e6d1d77ec8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': '  inferring social links from mobility profiles ',\n",
       " 'conf/mobisys/LiuLLW19': 'real time arm skeleton tracking and gesture inference tolerant to missing wearable sensors ',\n",
       " 'conf/icarcv/LlopartRAK18': 'online semantic segmentation and manipulation of objects in task intelligence for service robots ',\n",
       " 'conf/icinfa/Liu17': 'computational awareness for learning neural network ensembles ',\n",
       " 'conf/gi/MunteanuSSW19': 'on coresets for logistic regression '}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "Tg3yhb2D_Nnd"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"secondclean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "1DyQPOM2_Nne"
   },
   "outputs": [],
   "source": [
    "# on veut supprimer les références à des pays et ce qui reste après nettoyage\n",
    "\n",
    "from country_list import countries_for_language # pip install country_list\n",
    "\n",
    "countries = dict(countries_for_language('en'))\n",
    "country_exclusion = [x.lower() for x in list(countries.values())]\n",
    "country_exclusion = '|'.join(country_exclusion)\n",
    "\n",
    "for i in article_titles.copy():\n",
    "    article_titles[i]=re.sub(country_exclusion,' ',article_titles[i])\n",
    "    article_titles[i]=re.sub(r'\\b\\D{1,2}\\b',' ',article_titles[i]) # On supprime les mots de moins de 3 lettres qui ne contiennent pas de chiffres qui seraient resté\n",
    "    article_titles[i]=re.sub(' +', ' ', article_titles[i]) # suppression des espaces en trop à la fin du nettoyage\n",
    "    if (len(article_titles[i])<3) : # suppression des titres à 0 ou 1 mots\n",
    "        del article_titles[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': ' inferring social links from mobility profiles ',\n",
       " 'conf/mobisys/LiuLLW19': 'real time arm skeleton tracking and gesture inference tolerant missing wearable sensors ',\n",
       " 'conf/icarcv/LlopartRAK18': 'online semantic segmentation and manipulation objects task intelligence for service robots ',\n",
       " 'conf/icinfa/Liu17': 'computational awareness for learning neural network ensembles ',\n",
       " 'conf/gi/MunteanuSSW19': ' coresets for logistic regression '}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPwBTMlT_Nng"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "o3HRVQ-__Nng"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"nocountries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "nkIAqP04_Nni"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"nocountries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLjsJJvz_Nnk"
   },
   "source": [
    "#### <span style=\"color:orange\">----------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjWzqwox_Nnk"
   },
   "source": [
    "#### <span style=\"color:red\">Suppression des articles qui ne sont pas en anglais</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469506"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    if (article_titles[i].isdigit()==True):\n",
    "        print(article_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles.copy(): # test de la présence dans la chaine de l'unique présence de characteres spéciaux\n",
    "    # car cela peut causer une erreur dans le test de détection de la langue\n",
    "    if re.match(r'^[_\\W]+$',article_titles[i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "Za9bNpZZ_Nnk",
    "outputId": "e628c8dd-9a53-4958-a8b2-db2e4f8789d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Cette celulle prend beaucoup de temps ###   \n",
    "\n",
    "# On voudrait également supprimer les publications qui ne sont pas en langue anglaise\n",
    "\n",
    "from langdetect import detect_langs # pip install langdetect\n",
    "\n",
    "en_ratio = 0.5\n",
    "for k,v in article_titles.copy().items() :\n",
    "    res = detect_langs(v)\n",
    "    # Si le titre n'a aucune trace d'anglais\n",
    "    if not [l for l in res if l.lang == 'en'] :\n",
    "        del article_titles[k]\n",
    "    else :\n",
    "        for lang in res :\n",
    "            # Si le titre a des traces d'anglais mais pas suffisamment\n",
    "            if lang.lang == 'en' and lang.prob < en_ratio :\n",
    "                del article_titles[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439506"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_titles)# supression de 30 000 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': ' inferring social links from mobility profiles ',\n",
       " 'conf/mobisys/LiuLLW19': 'real time arm skeleton tracking and gesture inference tolerant missing wearable sensors ',\n",
       " 'conf/icarcv/LlopartRAK18': 'online semantic segmentation and manipulation objects task intelligence for service robots ',\n",
       " 'conf/icinfa/Liu17': 'computational awareness for learning neural network ensembles ',\n",
       " 'conf/gi/MunteanuSSW19': ' coresets for logistic regression '}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = [s for s in article_titles.values() if 'data science' in s] # test de la présence du mot conference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GheSL7K_NoK"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "ZYjytGdR_NoK"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"onlyenglish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "JrGaHQp4_NoM"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"onlyenglish\") # Les titres des publications sont ici les plus probables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps5VjSHv_NoO"
   },
   "source": [
    "#### <span style=\"color:orange\">----------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqK5jbeU_NoO"
   },
   "source": [
    "#### <span style=\"color:red\">Suppression des stop words</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "MTNbjhk8_NoO"
   },
   "outputs": [],
   "source": [
    "# tokenisation des titres\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokenized_titles = defaultdict(set)\n",
    "for k,v in article_titles.items() :\n",
    "    article_titles[k] = tokenizer.tokenize(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI1uX6vV_NoQ"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "OL54wqOP_NoR"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "o8y6aO0A_NoS"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oZ6lKAc_NoU"
   },
   "source": [
    "#### <span style=\"color:orange\">----------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "HixlVVdm_NoU"
   },
   "outputs": [],
   "source": [
    "# suppression des mots vides\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS # pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "l38qKvgO_NoW",
    "outputId": "dbc6dfa1-f9c1-4080-fd08-ccf3d9210b88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'towards', 'sixty', 'hereafter', 'fire', 'wherever', 'beyond', 'she', 'around', 'find', 'moreover', 'the', 'haven', 'that', 'bottom', 'some', 'beforehand', 'became', 'behind', 'or', 'anywhere', \"shan't\", \"aren't\", 'someone', 'isn', \"mustn't\", 'put', 'whatever', 'nine', 'your', 'none', 'fifty', 'enough', \"hasn't\", 'wasn', 'where', 'shouldn', 'too', 'are', 'needn', 'rather', 'which', 'nowhere', 'elsewhere', 'make', 'else', 'nothing', 'amoungst', 'm', 'several', 'latter', 'part', 've', 'd', \"you've\", \"she's\", 'one', 'there', 'mustn', 'after', 'afterwards', 'herein', 'must', 'couldnt', 'de', 'and', 'often', 'become', 'fill', 't', 'between', 'hundred', 'fifteen', 'see', 'once', 'forty', 'computer', 'mostly', 'who', 'didn', 'don', 'go', 'wouldn', 'as', 'seemed', 'what', 'same', 'back', 'had', 'out', 'y', 'whither', 'doesn', 'yet', 'please', 'take', 'system', 'regarding', 'he', 'therein', 'made', 'though', \"couldn't\", 'latterly', 'last', 'give', 'formerly', 'his', 'everywhere', \"it's\", 'although', 'perhaps', 'anyway', 'any', 'here', 'three', 'due', 'most', 'whereas', 'noone', 'whether', 'among', 'while', \"don't\", 'really', 'otherwise', 'about', 'four', 'ma', 'was', 'ltd', 'neither', 'yours', 'only', 'whenever', 'amongst', 'ourselves', 'mine', 'be', 'via', 'those', 'himself', \"hadn't\", \"mightn't\", 'am', \"wasn't\", 'within', 'up', 'seeming', 'namely', 'eight', 'hasn', 'thence', 'couldn', 'beside', 'whom', 'almost', 'serious', 'at', \"didn't\", 'if', 'over', 'will', 'except', \"that'll\", 'becomes', 'thus', 'down', 'next', 'no', 'very', 'may', 'our', 'have', 'whereby', 'been', 'before', \"should've\", 'five', 'upon', 'kg', 'during', 'describe', 're', \"you'd\", 'well', 'eg', 'should', 'we', 'because', 'cannot', 'anyhow', 'ie', 'thereafter', 'cant', 'full', 'again', 'others', 'cry', 'hereupon', 'these', 'however', 'more', 'interest', 'various', 'for', 'name', 'whence', \"haven't\", 'amount', 'six', 'its', 'through', 'always', 'on', 'whoever', 'so', 'off', 'front', 'us', 'less', 'were', 'would', 'former', 'get', 'thereupon', 'every', 'under', 'seem', 'few', 'aren', 'not', 'somehow', 'hers', 'o', \"needn't\", 'alone', 'hence', 'move', 'everything', 'all', 'per', 'nor', 'thin', 'into', 'them', 'an', 'it', 'done', 'why', 'thick', 'say', 'quite', 'has', 'third', 'twelve', 'un', 'anyone', 'won', 'inc', 'still', 'onto', 'him', 'a', 'km', 'used', 'their', 'myself', 'both', 'herself', 'toward', 'then', 'yourself', 'other', 'just', 'me', 'etc', 'co', 'you', 'theirs', 'mill', 'shan', 'indeed', 'ours', 'throughout', 'did', 'another', 'sometime', 'from', 'thru', 'with', 'does', 'own', \"you'll\", 'yourselves', 'her', 'to', 'becoming', 'until', 'wherein', 'also', 'first', 'having', 'how', 'now', 'even', \"shouldn't\", 'therefore', 'bill', 'doing', 'my', 'hasnt', \"won't\", 'ever', 'weren', 'much', 'con', 'sincere', \"doesn't\", \"weren't\", 'detail', 'never', 'nevertheless', 'two', 'when', 'mightn', 'anything', 'many', 'hadn', 'across', 'of', 'but', 'top', \"you're\", 'above', 'side', 'could', 'such', 'somewhere', 'below', 'further', 'already', 'sometimes', 'than', 'keep', 'together', 'empty', \"isn't\", 'ten', 'whose', 'thereby', 'without', 'meanwhile', 'found', 'ain', 'hereby', 'll', 'this', 'itself', 'against', 'least', 'themselves', 'nobody', 'eleven', 'whole', 'call', 'each', 'something', 'either', 'whereupon', \"wouldn't\", 'everyone', 'they', 'i', 'might', 'besides', 'using', 'is', 'twenty', 'being', 'can', 'along', 'unless', 's', 'do', 'since', 'in', 'by', 'whereafter', 'show', 'seems'})\n"
     ]
    }
   ],
   "source": [
    "# la suppression des mots vides est différente selon les différentes librairy. Il est néanmoins possible de modifier cette liste.\n",
    "\n",
    "stopwords = set()\n",
    "stopwords.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "all_stopwords = STOPWORDS.union(stopwords)\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "TxH6Immd_NoX"
   },
   "outputs": [],
   "source": [
    "for k,v in article_titles.items() :\n",
    "    article_titles[k] = [word for word in v if word not in all_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "xwtOiWmY_Nob",
    "outputId": "70989bb8-6d01-41da-e4ad-c683a740e2e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': ['inferring',\n",
       "  'social',\n",
       "  'links',\n",
       "  'mobility',\n",
       "  'profiles'],\n",
       " 'conf/mobisys/LiuLLW19': ['real',\n",
       "  'time',\n",
       "  'arm',\n",
       "  'skeleton',\n",
       "  'tracking',\n",
       "  'gesture',\n",
       "  'inference',\n",
       "  'tolerant',\n",
       "  'missing',\n",
       "  'wearable',\n",
       "  'sensors'],\n",
       " 'conf/icarcv/LlopartRAK18': ['online',\n",
       "  'semantic',\n",
       "  'segmentation',\n",
       "  'manipulation',\n",
       "  'objects',\n",
       "  'task',\n",
       "  'intelligence',\n",
       "  'service',\n",
       "  'robots'],\n",
       " 'conf/icinfa/Liu17': ['computational',\n",
       "  'awareness',\n",
       "  'learning',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'ensembles'],\n",
       " 'conf/gi/MunteanuSSW19': ['coresets', 'logistic', 'regression']}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching = [s for s in article_titles.values() if 'the' in s] # test de la présence du mot conference\n",
    "matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AeEF7o6_Noe"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "R77zHYyX_Noe"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"nostopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxgzR3la_Nog"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"nostopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XCdZl4d_Noi"
   },
   "source": [
    "#### <span style=\"color:orange\">----------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ejrmo_3_Noi"
   },
   "source": [
    "#### <span style=\"color:red\">Lemmatisation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTjmVD0l_Noj",
    "outputId": "c0640eda-d8a7-4c84-9ed6-8bd215bb6c21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Romain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "YOmj9b7A_Nok"
   },
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Fonction qui va servir à identité la nature du mot (verbe, nom, ...)\n",
    "def get_wordnet_pos(word) :\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tags = {'J' : wordnet.ADJ,\n",
    "            'N' : wordnet.NOUN,\n",
    "            'V' : wordnet.VERB,\n",
    "            'R' : wordnet.ADV}\n",
    "    return tags.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for k,v in article_titles.items() :\n",
    "    article_titles[k] = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': ['infer', 'social', 'link', 'mobility', 'profile'],\n",
       " 'conf/mobisys/LiuLLW19': ['real',\n",
       "  'time',\n",
       "  'arm',\n",
       "  'skeleton',\n",
       "  'track',\n",
       "  'gesture',\n",
       "  'inference',\n",
       "  'tolerant',\n",
       "  'miss',\n",
       "  'wearable',\n",
       "  'sensor'],\n",
       " 'conf/icarcv/LlopartRAK18': ['online',\n",
       "  'semantic',\n",
       "  'segmentation',\n",
       "  'manipulation',\n",
       "  'object',\n",
       "  'task',\n",
       "  'intelligence',\n",
       "  'service',\n",
       "  'robot'],\n",
       " 'conf/icinfa/Liu17': ['computational',\n",
       "  'awareness',\n",
       "  'learn',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'ensemble'],\n",
       " 'conf/gi/MunteanuSSW19': ['coresets', 'logistic', 'regression']}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Vw_YMJ_Nol"
   },
   "source": [
    "#### <span style=\"color:orange\">Sauvegarde</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "9pR59YnA_Non"
   },
   "outputs": [],
   "source": [
    "#save_dict(article_titles, \"lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "s2vgJQsS_Nop"
   },
   "outputs": [],
   "source": [
    "article_titles = load_dict(\"lemmatized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Suppression du mot base (fait suite au notebook sur la visualisation)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_titles:\n",
    "    for j in article_titles[i].copy(): # on parcours une copie pour éviter certaines erreurs d'index\n",
    "        if j==\"base\":\n",
    "            article_titles[i].remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf/ccs/0001HPZ17': ['infer', 'social', 'link', 'mobility', 'profile'],\n",
       " 'conf/mobisys/LiuLLW19': ['real',\n",
       "  'time',\n",
       "  'arm',\n",
       "  'skeleton',\n",
       "  'track',\n",
       "  'gesture',\n",
       "  'inference',\n",
       "  'tolerant',\n",
       "  'miss',\n",
       "  'wearable',\n",
       "  'sensor'],\n",
       " 'conf/icarcv/LlopartRAK18': ['online',\n",
       "  'semantic',\n",
       "  'segmentation',\n",
       "  'manipulation',\n",
       "  'object',\n",
       "  'task',\n",
       "  'intelligence',\n",
       "  'service',\n",
       "  'robot'],\n",
       " 'conf/icinfa/Liu17': ['computational',\n",
       "  'awareness',\n",
       "  'learn',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'ensemble'],\n",
       " 'conf/gi/MunteanuSSW19': ['coresets', 'logistic', 'regression']}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(article_titles.items())[0:5]) # affichage des 5 premiers éléments du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(article_titles, \"cleaned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
